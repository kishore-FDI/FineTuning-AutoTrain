{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgvqeBz_3XvO"
      },
      "outputs": [],
      "source": [
        "!pip install pandas autotrain-advanced opendatasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwStofw4257S"
      },
      "outputs": [],
      "source": [
        "!autotrain setup --update-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzMLmLP86Ub-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxTn4r_YZdkY"
      },
      "outputs": [],
      "source": [
        "!git clone https://huggingface.co/datasets/atharvamungee10/llama_2_fine_tune\n",
        "%cd llama_2_fine_tune\n",
        "%mv train.csv ../train.csv\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUb-rkeoZzZ6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "# df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEFbHxoPaDE_"
      },
      "source": [
        "##  Overview of AutoTrain command\n",
        "\n",
        "#### Short overview of what the command flags do.\n",
        "\n",
        "- `!autotrain`: Command executed in environments like a Jupyter notebook to run shell commands directly. `autotrain` is an automatic training utility.\n",
        "\n",
        "- `llm`: A sub-command or argument specifying the type of task\n",
        "\n",
        "- `--train`: Initiates the training process.\n",
        "\n",
        "- `--project_name`: Sets the name of the project\n",
        "\n",
        "- `--model abhishek/llama-2-7b-hf-small-shards`: Specifies original model that is hosted on Hugging Face named \"llama-2-7b-hf-small-shards\" under the \"abhishek\".\n",
        "\n",
        "- `--data_path .`: The path to the dataset for training. The \".\" refers to the current directory. The `train.csv` file needs to be located in this directory.\n",
        "\n",
        "- `--use_int4`: Use of INT4 quantization to reduce model size and speed up inference times at the cost of some precision.\n",
        "\n",
        "- `--learning_rate 2e-4`: Sets the learning rate for training to 0.0002.\n",
        "\n",
        "- `--block_size xxx` : To trucncate the data to fit the model input id requirements\n",
        "\n",
        "- `--train_batch_size 12`: Sets the batch size for training to 12.\n",
        "\n",
        "- `--num_train_epochs 3`: The training process will iterate over the dataset 3 times.\n",
        "\n",
        "### Steps needed before running\n",
        "Go to the `!autotrain` code cell below and update it by following the steps below:\n",
        "\n",
        "1. After `--project_name` replace `*enter-a-project-name*` with the name that you'd like to call the project\n",
        "2. After `--repo_id` replace `*username*/*repository*`. Replace `*username*` with your Hugging Face username and `*repository*` with the repository name you'd like it to be created under. You don't need to create this repository before hand, it will automatically be created and uploaded once the training is completed.\n",
        "3. Confirm that `train.csv` is in the root directory in the Colab. The `--data_path .` flag will make it so that AutoTrain looks for your data there.\n",
        "4. Make sure to add the LoRA Target Modules to be trained `--target-modules q_proj, v_proj`\n",
        "5. Once you've made these changes you're all set, run the command below!\n",
        "\n",
        "## --target_modules q_proj,v_proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFS31VJsZ-pa"
      },
      "outputs": [],
      "source": [
        "!autotrain llm \\\n",
        "--train \\\n",
        " --project_name mistral-7b-mj-finetuned \\\n",
        " --model bn22/Mistral-7B-Instruct-v0.1-sharded \\\n",
        "  --data_path . \\\n",
        "  --use_peft \\\n",
        "  --use_int4 \\\n",
        "  --learning_rate 2e-4 \\\n",
        "  --train_batch_size 2 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --trainer sft \\\n",
        "  --target_modules q_proj,v_proj \\\n",
        " --push_to_hub \\\n",
        " --repo_id lkjhgfdsaadvgdsvsdvad/mistral-finetuned-test4 \\\n",
        " --token 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 \\\n",
        " --block_size 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIoxuAEAfJ4z"
      },
      "source": [
        "## Step 6: Inference Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYsYyXmrc0xu"
      },
      "outputs": [],
      "source": [
        "# !autotrain llm -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5m1ouhWhc2fr"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft  accelerate bitsandbytes safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8s-nDnnPc--U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "adapters_name = \"lkjhgfdsaadvgdsvsdvad/mistral-finetuned-test4\"\n",
        "model_name = \"bn22/Mistral-7B-Instruct-v0.1-sharded\" #\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "\n",
        "device = \"cuda\" # the device to load the model onto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HosPywN_dEpl"
      },
      "outputs": [],
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtZx4CZUdt1f"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh5Xc0clfQkZ"
      },
      "source": [
        "## Step 7: Peft Model Loading with upload model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Rt6sOPFVdvWX"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, adapters_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3OArVILeoZH"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.bos_token_id = 1\n",
        "\n",
        "stop_token_ids = [0]\n",
        "\n",
        "print(f\"Successfully loaded the model {model_name} into memory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbOOX8cve0lR",
        "outputId": "f21d3af9-56a8-459c-fe8f-3e66593eefca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST] how to write a program to count till infinity   [/INST] Here is a simple Python program that counts from 1 to infinity using an infinite loop:\n",
            "```\n",
            "# Infinite loop\n",
            "for i in range(1,):\n",
            "    print(i)\n",
            "```\n",
            "This program uses the `range()` function which creates a sequence of numbers from start to stop (not included) and increments the stop number each iteration. Here, the start of the range is 1 and the stop value is excluded, so the sequence continues forever.</s>\n"
          ]
        }
      ],
      "source": [
        "text = \"[INST] how to write a program to count till infinity   [/INST]\"\n",
        "\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded.to('cuda')\n",
        "model.to(device)\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}